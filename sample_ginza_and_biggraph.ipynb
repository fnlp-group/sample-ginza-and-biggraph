{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample-ginza-and-biggraph.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuukimiyo/sample-ginza-and-biggraph/blob/master/sample_ginza_and_biggraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYE-29j9eaZk",
        "colab_type": "text"
      },
      "source": [
        "# リクルートのGiNZAとBiggraphを用いて萩原朔太郎の詩を分析するサンプルコード\n",
        "\n",
        "リクルートが公開している自然言語処理ライブラリのGiNZAを用いて用意した単語間の係り受け関係を、Biggraphを用いて処理するためのサンプルコードです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBJc8PRU4IEk",
        "colab_type": "text"
      },
      "source": [
        "# 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzjjyI5j_0FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir download\n",
        "!mkdir -p data/sakutarou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUan0gVDirwh",
        "colab_type": "text"
      },
      "source": [
        "## 学習データを取得\n",
        "青空文庫からテキストをダウンロードして解凍"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvZWz8ye__6T",
        "colab_type": "code",
        "outputId": "6dad8168-8826-425c-c4f6-7321e1baaefa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 月に吠える(1917)\n",
        "!wget -qP download https://www.aozora.gr.jp/cards/000067/files/859_ruby_21655.zip\n",
        "\n",
        "# 青猫(1923)\n",
        "!wget -qP download https://www.aozora.gr.jp/cards/000067/files/1768_ruby_18600.zip\n",
        "\n",
        "# 蝶を夢む(1924)\n",
        "!wget -qP download https://www.aozora.gr.jp/cards/000067/files/1769_ruby_18601.zip\n",
        "\n",
        "# 純情小曲集(1925)\n",
        "!wget -qP download https://www.aozora.gr.jp/cards/000067/files/1788_ruby_18602.zip\n",
        "\n",
        "# 氷島(1934)\n",
        "!wget -qP download https://www.aozora.gr.jp/cards/000067/files/4869_ruby_14055.zip\n",
        "\n",
        "# すべてのZipファイルを解凍\n",
        "!unzip -q 'download/*.zip' -d data/sakutarou \"*.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IBP5wErc5_P",
        "colab_type": "text"
      },
      "source": [
        "## GiNZAをインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yZciSqw4OrC",
        "colab_type": "code",
        "outputId": "1587166a-6788-4b1e-8938-ce248fbf1f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!pip -q install \"https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.2MB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 54.6MB 824kB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 24.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 70.7MB 512kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 27.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 32.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 49.9MB/s \n",
            "\u001b[?25h  Building wheel for ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ja-ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for SudachiDict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4YuEemOr5yQ",
        "colab_type": "text"
      },
      "source": [
        "***ここで「ランタイムを再起動」***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vicrw9mii848",
        "colab_type": "text"
      },
      "source": [
        "## GiNZAの係り受け関係の日本語変換用辞書の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-RJeMIVaQxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddic = {\n",
        "    \"acl\": \"名詞の節修飾語\",\n",
        "    \"advcl\": \"副詞節修飾語\",\n",
        "    \"advmod\": \"副詞修飾語\",\n",
        "    \"amod\": \"形容詞修飾語\",\n",
        "    \"appos\": \"同格\",\n",
        "    \"aux\": \"助動詞\",\n",
        "    \"case\": \"格表示\",\n",
        "    \"cc\": \"等位接続詞\",\n",
        "    \"ccomp\": \"補文\",\n",
        "    \"clf\": \"類別詞\",\n",
        "    \"compound\": \"複合名詞\",\n",
        "    \"conj\": \"結合詞\",\n",
        "    \"cop\": \"連結詞\",\n",
        "    \"csubj\": \"主部\",\n",
        "    \"dep\": \"不明な依存関係\",\n",
        "    \"det\": \"限定詞\",\n",
        "    \"discourse\": \"談話要素\",\n",
        "    \"dislocated\": \"転置\",\n",
        "    \"expl\": \"嘘辞\",\n",
        "    \"fixed\": \"固定複数単語表現\",\n",
        "    \"flat\": \"同格複数単語表現\",\n",
        "    \"goeswith\": \"1単語分割表現\",\n",
        "    \"iobj\": \"関節目的語\",\n",
        "    \"list\": \"リスト表現\",\n",
        "    \"mark\": \"接続詞\",\n",
        "    \"nmod\": \"名詞修飾語\",\n",
        "    \"nsubj\": \"主語名詞\",\n",
        "    \"nummod\": \"数詞修飾語\",\n",
        "    \"obj\": \"目的語\",\n",
        "    \"obl\": \"斜格名詞\",\n",
        "    \"orphan\": \"独立関係\",\n",
        "    \"parataxis\": \"並列\",\n",
        "    \"punct\": \"句読点\",\n",
        "    \"reparandum\": \"単語として認識されない単語表現\",\n",
        "    \"root\": \"ROOT\",\n",
        "    \"vocative\": \"発声関係\",\n",
        "    \"xcomp\": \"補体\"\n",
        "}\n",
        "\n",
        "pdic = {\n",
        "    \"ADJ\": \"形容詞\",\n",
        "    \"ADP\": \"設置詞\",\n",
        "    \"ADV\": \"副詞\",\n",
        "    \"AUX\": \"助動詞\",\n",
        "    \"CCONJ\": \"接続詞\",\n",
        "    \"DET\": \"限定詞\",\n",
        "    \"INTJ\": \"間投詞\",\n",
        "    \"NOUN\": \"名詞\",\n",
        "    \"NUM\": \"数詞\",\n",
        "    \"PART\": \"助詞\",\n",
        "    \"PRON\": \"代名詞\",\n",
        "    \"PROPN\": \"固有名詞\",\n",
        "    \"PUNCT\": \"句読点\",\n",
        "    \"SCONJ\": \"連結詞\",\n",
        "    \"SYM\": \"シンボル\",\n",
        "    \"VERB\": \"動詞\",\n",
        "    \"X\": \"その他\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZRHwF7WEwGG",
        "colab_type": "text"
      },
      "source": [
        "# メイン処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0KV9Jj67Zps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import codecs\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtGwn4VlrLeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 対象言語セットを日本語(GiNZA)としてSpyCyを初期化\n",
        "nlp = spacy.load('ja_ginza')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d12yO4TYsjob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 読み込む書籍データを作成\n",
        "books = [\n",
        "    # タイトル, ファイル, 読込開始行, 読込終了行\n",
        "    (\"月に吠える\", \"data/sakutarou/02tsukini_hoeru.txt\", 138, 1172),\n",
        "    (\"青猫\", \"data/sakutarou/aoneko.txt\", 71, 1547),\n",
        "    (\"蝶を夢む\", \"data/sakutarou/choo_yumemu.txt\", 47, 1281),\n",
        "    (\"純情小曲集\", \"data/sakutarou/02junjo_shokyokushu.txt\", 73, 538),\n",
        "    (\"氷島\", \"data/sakutarou/hyoto.txt\", 40, 566)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaA91G8Tjor3",
        "colab_type": "text"
      },
      "source": [
        "## データのクレンジング処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HgS_6gzEsUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# クレンジング関数を作成\n",
        "\n",
        "# 正規表現置き換えのパターン作成\n",
        "ptns = []\n",
        "ptns.append(re.compile(r\"＃「」\"))\n",
        "ptns.append(re.compile(r\"\\\\+n\"))\n",
        "ptns.append(re.compile(r\"[\\s]+\"))\n",
        "ptns.append(re.compile(r\"※\"))\n",
        "ptns.append(re.compile(r\"#[!-~]+\"))\n",
        "ptns.append(re.compile(r\"［[^］]*］\")) # 青空文庫注釈の削除\n",
        "ptns.append(re.compile(r\"《[^》]*》\")) # 青空文庫ルビの削除\n",
        "\n",
        "def cleanText(t):\n",
        "    # 前後の改行等削除\n",
        "    t = t.strip()\n",
        "    \n",
        "    # 正規表現による置き換えを実行\n",
        "    for ptn in ptns:\n",
        "        t = ptn.sub(\"\", t)\n",
        "    \n",
        "    return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHcJF63wJQSQ",
        "colab_type": "code",
        "outputId": "6e99cd41-9933-4b60-9626-34fd7cc8faac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# クレンジング処理を実施\n",
        "lines = []\n",
        "for book in books:\n",
        "    with codecs.open(book[1], 'r', encoding='cp932') as f:\n",
        "        for i, l in enumerate(f.readlines()):\n",
        "\n",
        "            # 前文を削除\n",
        "            if i < book[2]:\n",
        "                continue;\n",
        "\n",
        "            # あとがき等を削除\n",
        "            if i > book[3]:\n",
        "                break;\n",
        "\n",
        "            l = cleanText(l)\n",
        "            if len(l) > 3:\n",
        "                lines.append(l)\n",
        "\n",
        "len(lines)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3466"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar41Ol7alKJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# クレンジング前\n",
        "with codecs.open('./data/sakutarou/02tsukini_hoeru.txt', 'r', encoding='cp932') as f:\n",
        "    for i, l in enumerate(f.readlines()):\n",
        "        print(l)\n",
        "        if i > 40:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFsiaUwHk5Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# クレンジング後\n",
        "lines[0:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc5NVm-pj-II",
        "colab_type": "text"
      },
      "source": [
        "## トークナイズ／Embedding処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZYC9TrtkinM",
        "colab_type": "text"
      },
      "source": [
        "### トークナイザ更新用関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GfI-_2R6fdXo",
        "colab": {}
      },
      "source": [
        "def filter_spans(spans):\n",
        "    # Tokenizerの更新に必要なフレーズ配列を生成する\n",
        "    \n",
        "    # 文字列の長さ（より長いもの）、文中の位置（より後ろのもの）でソート\n",
        "    get_sort_key = lambda span: (span.end - span.start, span.start)\n",
        "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
        "    result = []\n",
        "    seen_tokens = set()\n",
        "    for span in sorted_spans:\n",
        "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
        "            result.append(span)\n",
        "            seen_tokens.update(range(span.start, span.end))\n",
        "    return result\n",
        "\n",
        "def update_tokenizer(doc):\n",
        "    # Tokenizerを固有表現/名詞区区切りに更新する\n",
        "    \n",
        "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
        "    spans = filter_spans(spans)\n",
        "\n",
        "    # 単語の区切りを、固有表現や名詞句の塊で置き換える\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in spans:\n",
        "            retokenizer.merge(span)\n",
        "    return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azjGfUnYrD_H",
        "colab_type": "code",
        "outputId": "9f2f5609-4c83-4234-8aca-e065a0541240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenizer更新の確認\n",
        "text = \"東京エリアの日本武道館\"\n",
        "    \n",
        "# doc_tmp = nlp(text)\n",
        "doc_tmp = update_tokenizer(doc_tmp)\n",
        "\n",
        "for ent in doc_tmp:\n",
        "    print('{:<30}{}'.format(ent.text, ent.dep_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "東京エリア                         nmod\n",
            "の                             case\n",
            "日本武道館                         ROOT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jayQhnfhmB_H",
        "colab_type": "text"
      },
      "source": [
        "## 学習用データの作成\n",
        "\n",
        "更新したトークナイザを利用し、単語を抽出しながら係り受け関係を取得する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNl9eC7DQ_oj",
        "colab_type": "code",
        "outputId": "fdb9216a-0b06-43fc-9c9e-0ada77d7d81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "source_list = []\n",
        "result = []\n",
        "for line in tqdm(lines):\n",
        "    doc = nlp(line)\n",
        "    \n",
        "    # トークナイザーを更新（固有表現抽出）\n",
        "    doc = update_tokenizer(doc)\n",
        "    \n",
        "    for token in doc:\n",
        "        \n",
        "        # 係り元と係り先が同じ場合は無視\n",
        "        if token.text == token.head.text:\n",
        "            continue\n",
        "        \n",
        "        # 特定の係り受け関係は無視\n",
        "        # if token.dep_ in (\"punct\", \"case\", \"cc\"):\n",
        "        #     continue\n",
        "\n",
        "        # 特定の品詞は無視\n",
        "        if token.pos_ in (\"AUX\", \"INTJ\", \"PUNCT\", \"ADP\", \"CCONJ\", \"PART\", \"SCONJ\"):\n",
        "            continue\n",
        "\n",
        "        source_list.append(line)\n",
        "        result.append(\"{}\\t{}\\t{}\".format(token.lemma_, ddic[token.dep_.lower()], token.head.lemma_))\n",
        "        # print(token.text, token.dep_, ddic[token.dep_.lower()], token.tag_, token.pos_, pdic[token.pos_], pdic[token.head.pos_], token.head.text, [child for child in token.children])\n",
        "\n",
        "len(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3466/3466 [00:53<00:00, 65.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_2bK-YmgYq",
        "colab_type": "text"
      },
      "source": [
        "### 抽出した係り受け関係を保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2w8dhVLRzZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with codecs.open('data/sakutarou_train.tsv', mode='w', encoding='utf-8') as f:\n",
        "    for t in result:\n",
        "        f.write(t + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU7L7Lqeygo_",
        "colab_type": "text"
      },
      "source": [
        "# BigGraph関連"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By4ivvjn9Ypa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -q install torchbiggraph\n",
        "!pip -q install pyflann\n",
        "!pip -q install 2to3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DsiVcOUm7OD",
        "colab_type": "text"
      },
      "source": [
        "上でインストールしたpyflannはPython3だとエラーが出たので、2to3でコードを修正する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbtdy6gdmw5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!2to3 -w /usr/local/lib/python3.6/dist-packages/pyflann"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T29LNYUpIiL",
        "colab_type": "text"
      },
      "source": [
        "### Biggraph用の設定ファイルを作成して保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcnnr4LQ8uqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_file_string = '''#!/usr/bin/env python3\n",
        "\n",
        "def get_torchbiggraph_config():\n",
        "\n",
        "    config = dict(\n",
        "        # I/O data\n",
        "        entity_path='entity',\n",
        "        edge_paths=[\n",
        "            'entity/sakutarou_train_partitioned'\n",
        "            ],\n",
        "        checkpoint_path='model',\n",
        "\n",
        "        # Graph structure\n",
        "        entities={\n",
        "            'all': {'num_partitions': 1},\n",
        "        },\n",
        "        relations=[{\n",
        "            'name': 'all_edges',\n",
        "            'lhs': 'all',\n",
        "            'rhs': 'all',\n",
        "            'operator': 'complex_diagonal',\n",
        "        }],\n",
        "        dynamic_relations=True,\n",
        "\n",
        "        # Scoring model\n",
        "        dimension=400,\n",
        "        global_emb=False,\n",
        "        comparator='dot',\n",
        "\n",
        "        # Training\n",
        "        num_epochs=10,\n",
        "        num_uniform_negs=1000,\n",
        "        loss_fn='softmax',\n",
        "        lr=0.1,\n",
        "\n",
        "        # Evaluation during training\n",
        "        eval_fraction=0,  # to reproduce results, we need to use all training data\n",
        "    )\n",
        "\n",
        "    return config'''\n",
        "\n",
        "with codecs.open('conf.py', mode='w') as f:\n",
        "    f.write(config_file_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlL5dUjfn4iN",
        "colab_type": "text"
      },
      "source": [
        "### Biggraph用のデータ作成ツールを使用して学習用データを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O045-QBo8pzj",
        "colab_type": "code",
        "outputId": "3bafcd1f-e494-4cca-d6ca-40365c2cd34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!torchbiggraph_import_from_tsv --lhs-col=0 --rel-col=1 --rhs-col=2 conf.py data/sakutarou_train.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking up relation types in the edge files...\n",
            "- Found 20 relation types\n",
            "- Removing the ones with fewer than 1 occurrences...\n",
            "- Left with 20 relation types\n",
            "- Shuffling them...\n",
            "Searching for the entities in the edge files...\n",
            "Entity type all:\n",
            "- Found 4113 entities\n",
            "- Removing the ones with fewer than 1 occurrences...\n",
            "- Left with 4113 entities\n",
            "- Shuffling them...\n",
            "Preparing counts and dictionaries for entities and relation types:\n",
            "- Writing count of entity type all and partition 0\n",
            "- Writing count of dynamic relations\n",
            "Preparing edge path entity/sakutarou_train_partitioned, out of the edges found in data/sakutarou_train.tsv\n",
            "- Edges will be partitioned in 1 x 1 buckets.\n",
            "- Processed 11757 edges in total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF51z6GxpZjJ",
        "colab_type": "text"
      },
      "source": [
        "### 作成したデータを使用して学習処理を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuRg5NBe9ecA",
        "colab_type": "code",
        "outputId": "7b709cc9-3f2a-45a0-c331-55db3d270bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!torchbiggraph_train conf.py -p edge_paths=entity/sakutarou_train_partitioned"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-18 06:55:14,513   [Trainer-0] Loading entity counts...\n",
            "2019-10-18 06:55:14,514   [Trainer-0] Creating workers...\n",
            "2019-10-18 06:55:14,560   [Trainer-0] Initializing global model...\n",
            "2019-10-18 06:55:14,684   [Trainer-0] Starting epoch 1 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:14,684   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:14,684   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:14,685   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:14,685   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:21,379   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 6.69 s ( 0.0018 M/sec ); io: 0.01 s ( 31.32 MB/sec )\n",
            "2019-10-18 06:55:21,379   [Trainer-0] ( 0 , 0 ): loss:  13.5788 , violators_lhs:  439.009 , violators_rhs:  453.191 , count:  11757\n",
            "2019-10-18 06:55:21,379   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:21,379   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:21,379   [Trainer-0] Finished epoch 1 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:21,397   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:21,403   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:21,403   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:21,403   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:21,409   [Trainer-0] Starting epoch 2 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:21,409   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:21,409   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:21,409   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:21,409   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:25,340   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 3.93 s ( 0.003 M/sec ); io: 0.00 s ( 106.28 MB/sec )\n",
            "2019-10-18 06:55:25,340   [Trainer-0] ( 0 , 0 ): loss:  8.88802 , violators_lhs:  37.8007 , violators_rhs:  37.0325 , count:  11757\n",
            "2019-10-18 06:55:25,340   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:25,340   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:25,340   [Trainer-0] Finished epoch 2 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:25,357   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:25,362   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:25,363   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:25,363   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:25,369   [Trainer-0] Starting epoch 3 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:25,370   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:25,370   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:25,370   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:25,370   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:29,425   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 4.05 s ( 0.0029 M/sec ); io: 0.00 s ( 103.74 MB/sec )\n",
            "2019-10-18 06:55:29,426   [Trainer-0] ( 0 , 0 ): loss:  5.3729 , violators_lhs:  5.20558 , violators_rhs:  4.70239 , count:  11757\n",
            "2019-10-18 06:55:29,426   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:29,426   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:29,426   [Trainer-0] Finished epoch 3 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:29,436   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:29,441   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:29,441   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:29,441   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:29,449   [Trainer-0] Starting epoch 4 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:29,449   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:29,449   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:29,449   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:29,449   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:33,433   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 3.98 s ( 0.003 M/sec ); io: 0.00 s ( 96.89 MB/sec )\n",
            "2019-10-18 06:55:33,433   [Trainer-0] ( 0 , 0 ): loss:  3.6399 , violators_lhs:  2.59029 , violators_rhs:  2.34618 , count:  11757\n",
            "2019-10-18 06:55:33,433   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:33,433   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:33,433   [Trainer-0] Finished epoch 4 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:33,442   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:33,447   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:33,448   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:33,448   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:33,454   [Trainer-0] Starting epoch 5 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:33,454   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:33,455   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:33,455   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:33,455   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:37,413   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 3.96 s ( 0.003 M/sec ); io: 0.00 s ( 106.91 MB/sec )\n",
            "2019-10-18 06:55:37,414   [Trainer-0] ( 0 , 0 ): loss:  2.98047 , violators_lhs:  2 , violators_rhs:  1.88339 , count:  11757\n",
            "2019-10-18 06:55:37,414   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:37,414   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:37,414   [Trainer-0] Finished epoch 5 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:37,424   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:37,429   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:37,429   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:37,430   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:37,436   [Trainer-0] Starting epoch 6 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:37,437   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:37,437   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:37,437   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:37,437   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:41,451   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 4.01 s ( 0.0029 M/sec ); io: 0.00 s ( 104.83 MB/sec )\n",
            "2019-10-18 06:55:41,451   [Trainer-0] ( 0 , 0 ): loss:  2.69776 , violators_lhs:  1.82181 , violators_rhs:  1.67534 , count:  11757\n",
            "2019-10-18 06:55:41,451   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:41,451   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:41,451   [Trainer-0] Finished epoch 6 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:41,460   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:41,466   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:41,466   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:41,466   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:41,473   [Trainer-0] Starting epoch 7 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:41,473   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:41,473   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:41,473   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:41,473   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:45,357   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 3.88 s ( 0.003 M/sec ); io: 0.00 s ( 101.82 MB/sec )\n",
            "2019-10-18 06:55:45,358   [Trainer-0] ( 0 , 0 ): loss:  2.49912 , violators_lhs:  1.64983 , violators_rhs:  1.59607 , count:  11757\n",
            "2019-10-18 06:55:45,358   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:45,358   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:45,358   [Trainer-0] Finished epoch 7 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:45,368   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:45,373   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:45,373   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:45,373   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:45,380   [Trainer-0] Starting epoch 8 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:45,380   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:45,380   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:45,380   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:45,380   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:49,294   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 3.91 s ( 0.003 M/sec ); io: 0.00 s ( 89.39 MB/sec )\n",
            "2019-10-18 06:55:49,294   [Trainer-0] ( 0 , 0 ): loss:  2.41174 , violators_lhs:  1.61504 , violators_rhs:  1.57472 , count:  11757\n",
            "2019-10-18 06:55:49,294   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:49,294   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:49,294   [Trainer-0] Finished epoch 8 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:49,303   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:49,309   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:49,309   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:49,309   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:49,315   [Trainer-0] Starting epoch 9 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:49,315   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:49,316   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:49,316   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:49,316   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:53,325   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 4.01 s ( 0.0029 M/sec ); io: 0.00 s ( 108.01 MB/sec )\n",
            "2019-10-18 06:55:53,326   [Trainer-0] ( 0 , 0 ): loss:  2.3201 , violators_lhs:  1.57974 , violators_rhs:  1.53704 , count:  11757\n",
            "2019-10-18 06:55:53,326   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:53,326   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:53,326   [Trainer-0] Finished epoch 9 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:53,342   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:53,348   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:53,348   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:53,348   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:53,355   [Trainer-0] Starting epoch 10 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:53,355   [Trainer-0] Edge path: entity/sakutarou_train_partitioned\n",
            "2019-10-18 06:55:53,355   [Trainer-0] still in queue: 0\n",
            "2019-10-18 06:55:53,355   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
            "2019-10-18 06:55:53,355   [Trainer-0] ( 0 , 0 ): Loading entities\n",
            "2019-10-18 06:55:57,382   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 11757 edges in 4.02 s ( 0.0029 M/sec ); io: 0.00 s ( 106.98 MB/sec )\n",
            "2019-10-18 06:55:57,382   [Trainer-0] ( 0 , 0 ): loss:  2.26163 , violators_lhs:  1.58051 , violators_rhs:  1.51016 , count:  11757\n",
            "2019-10-18 06:55:57,382   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
            "2019-10-18 06:55:57,382   [Trainer-0] Writing partitioned embeddings\n",
            "2019-10-18 06:55:57,382   [Trainer-0] Finished epoch 10 / 10, edge path 1 / 1, edge chunk 1 / 1\n",
            "2019-10-18 06:55:57,392   [Trainer-0] Writing the metadata\n",
            "2019-10-18 06:55:57,397   [Trainer-0] Writing the training stats\n",
            "2019-10-18 06:55:57,397   [Trainer-0] Writing the checkpoint\n",
            "2019-10-18 06:55:57,398   [Trainer-0] Switching to the new checkpoint version\n",
            "2019-10-18 06:55:57,653   [Trainer-0] Exiting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUIARZH1p3C1",
        "colab_type": "text"
      },
      "source": [
        "# モデルを利用して類似単語を抽出する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMrGb7XjLLA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "\n",
        "import h5py\n",
        "from pyflann import FLANN\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhWVTBauqZzr",
        "colab_type": "text"
      },
      "source": [
        "### 結果確認用の辞書（単語ID -> 単語文字列）を作成\n",
        "\n",
        "Biggraphの学習済みモデルは内部で単語をID化して扱っている。\n",
        "ID化された出力を人が理解できる単語文字列に変換するための辞書を、Biggraphの学習用データ作成ツールが作成したjsonファイルから生成する（こういったjsonファイルは普通どの実装でも作成される）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO9vGlscqWjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"entity/entity_names_all_0.json\") as f:\n",
        "    dictionary = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynXI-xrvrH2I",
        "colab_type": "text"
      },
      "source": [
        "### Biggraphによる学習済みモデルファイルを読み込む\n",
        "\n",
        "h5形式については次など参照<br />\n",
        "https://qiita.com/simonritchie/items/23db8b4cb5c590924d95"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJ3uf4nqN8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"model/embeddings_all_0.v10.h5\", \"r\") as f:\n",
        "    embeddings = f[\"embeddings\"][:, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibjkuhv2r0r1",
        "colab_type": "text"
      },
      "source": [
        "ベクトル距離を扱うためのライブラリ？、flannに読み込んだモデルデータを渡す。<br />\n",
        "各用語については次など参照になるかも<br />\n",
        "https://myenigma.hatenablog.com/entry/2016/04/19/214813"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLEGpozsrwPZ",
        "colab_type": "code",
        "outputId": "6b775bef-e916-46ac-ae11-662a3e4b9b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "flann = FLANN()\n",
        "flann.build_index(embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'algorithm': 'kdtree',\n",
              " 'branching': 32,\n",
              " 'build_weight': 0.009999999776482582,\n",
              " 'cb_index': 0.5,\n",
              " 'centers_init': 'random',\n",
              " 'checks': 32,\n",
              " 'eps': 0.0,\n",
              " 'iterations': 5,\n",
              " 'leaf_max_size': 4,\n",
              " 'log_level': 'warning',\n",
              " 'memory_weight': 0.0,\n",
              " 'random_seed': 997840035,\n",
              " 'sample_fraction': 0.10000000149011612,\n",
              " 'speedup': 0.0,\n",
              " 'target_precision': 0.8999999761581421,\n",
              " 'trees': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ozz9nltL4T",
        "colab_type": "text"
      },
      "source": [
        "flannを使って、指定した単語の類似単語を出力する関数を作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGdRaIFiEzrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search(word_str):\n",
        "    \n",
        "    # 入力単語が辞書に無い場合は停止\n",
        "    if word_str not in dictionary:\n",
        "        print(\"Dictionary doesn't have {}.\")\n",
        "        return\n",
        "    \n",
        "    # 入力単語をbiggraphのモデルで使用されているIDに変換\n",
        "    word_index = dictionary.index(word_str)\n",
        "    \n",
        "    # flannを使用して距離の近い単語を取得（しながら表示）\n",
        "    for rank, result_index in enumerate(flann.nn_index(embeddings[word_index], num_neighbors=10)[0][0]):\n",
        "        result_title = dictionary[result_index]\n",
        "        print(rank, \" \", result_title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsRlvw_-wDxb",
        "colab_type": "code",
        "outputId": "401ec95f-5e6f-4678-a12f-cf3e4cfcddef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "search(\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0   人\n",
            "1   ところ\n",
            "2   子供\n",
            "3   生涯\n",
            "4   私\n",
            "5   夜風\n",
            "6   そば\n",
            "7   書生\n",
            "8   火\n",
            "9   通り\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28dcLveowQGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search(\"竹\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e4UD6I7wZwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search(\"悠々\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVUJMESGQ3hT",
        "colab_type": "code",
        "outputId": "aed2a7fb-2fed-4b61-cacf-0d66e41b4c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "search(\"妹\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0   妹\n",
            "1   疱瘡\n",
            "2   けた幽靈\n",
            "3   女の子\n",
            "4   親鳥\n",
            "5   栗鼠\n",
            "6   絹いと\n",
            "7   日傭人\n",
            "8   むぐらもち\n",
            "9   齲ばみ酢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3rLwF2-vyuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search(\"死\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1fH7f3wt_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search(\"死\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}